<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">

  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Accurate Subtitle Generation in videos </h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">TranscribeAI</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <!--
          <div class="author-image">
                        
              <img src="">
            
            
          </div>-->
          <p>
                        
            Ruizi Wang 
            
          </p>
        </div>
        
        <div class="author-container">
          <!--
          <div class="author-image">
                        
              <img src="">
            
            
          </div>-->
          <p>
            
            Anwesha Samaddar
            
          </p>
        </div>
        <div class="author-container">
        <!--
          <div class="author-image">
                        
              <img src="">
            
            
          </div>-->
          <p>
            Sharan Rajamanoharan
          </p>
        </div>
        
        <div class="author-container">
          <!--
          <div class="author-image">
                        
              <img src="">
            
            
          </div>-->
          <p>
            Shizra Tariq 
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span> -->
            <!-- </a>
          </span> -->
          <span class="link-block">
            <a
              href="https://github.com/Ether9t/subtitle-helper"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span> -->
            <!-- /              -->
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Accurate captions for videos are essential for accessibility and understanding, especially as video content grows. 
  Current speech-to-text (STT) systems often have trouble predicting timestamps correctly, which results in misaligned or overly long captions.
We aim to improve readability of captions by accurately segmenting the audio transcripts generated by Whisper. We are using prompt engineering with LLMs to improve the segmentation and then realigning captions with word-level timestamps produced by Whisper-timestamped.</p>

<hr>

<h2 id="teaser">Teaser Figure</h2>

<p>The below image conveys the main reason for us doing this research. <a href="https://arxiv.org/pdf/2303.00747">StyLEx</a>.</p>

<p class="sys-img"><img src="./files/fig2.jpeg" alt="imgname"></p>

<h3 id="the-timeline-and-the-highlights">Meaning Behind this Image</h3>

<p>The caption given is clutterd and hard to read and is delayed leading to the overall experience of the viewer tanking. we would like to improve these aspects through our research.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>
<p>
Speech-to-Text (STT) systems often encounter challenges in generating accurate captions, particularly in scenarios involving rapid speech, diverse accent backgrounds, multi-speaker interactions. These challenges impact the readability and comprehension of video subtitles, especially in educational and entertainment contexts. This research aims to enhance subtitle generation and accuracy by developing advanced transcription techniques that improve the overall quality of captions across diverse platforms.
</p>

<hr>

<h2 id="objectives">Objectives</h2>
<p>
This research aims to enhance video subtitle alignment and segmentation for better accessibility and viewing experiences. Key objectives include:
</p>
<ul>
  <li>Developing advanced transcription models for accurate captions in complex audio.</li>
  <li>Implementing text segmentation techniques with state-of-the-art language models.</li>
  <li>Creating a robust methodology to validate caption quality across content types.</li>
  <li>Enhancing caption generation without altering original video timestamps.</li>
</ul>

<hr>

<h2 id="approach">Approach</h2>

<h3>Dataset and Pre-Processing</h3>

<h4 style="text-align: left;">Custom Dataset:</h4>
<p>
  We constructed a custom dataset from YouTube videos created by Euro Brady, which focus on analyzing the game Disco Elysium. The dataset comprises:
</p>
<ul>
  <li><b>13 long videos</b> (1-2 hours each): Offering in-depth analysis of the game</li>
  <li><b>3 short videos</b> (20-30 minutes each): Providing concise insights</li>
</ul>

<h4 style="text-align: left;">Pre-Processing:</h4>
<p>
  To prepare the dataset, we used automated transcription tools to generate .srt files, which were subsequently refined manually by:
</p>
<ul>
  <li>Aligning timestamps with spoken content for precise synchronization</li>
  <li>Correcting errors in proper nouns, grammar, and punctuation to enhance transcription accuracy</li>
</ul>

<p>This process ensured a high-quality dataset for evaluating and improving subtitle generation techniques.</p>

<h3>Methodology</h3>
<p>
  In our approach, we compared the ground truth (GT) captions with transcription outputs from three different variants of the Whisper model:
</p>
<ul>
  <li>Whisper-X</li>
  <li>Whisper-Timestamped</li>
  <li>Whisper-Medium</li>
</ul>
<p>
  To improve the performance of these models, we fine-tuned the Whisper base models on the <b>PolyAI/minds 14 dataset</b>, which consists of a large collection of conversational speech data from diverse sources. This dataset includes over 14,000 hours of dialogue across multiple domains and accents, making it an ideal resource for training a more robust speech-to-text (STT) system. 
</p>
<p>
  Fine-tuning on this dataset enabled Whisper to better handle various speech patterns, accents, and noise in real-world audio, leading to more accurate transcriptions, especially in noisy environments.
</p>

<hr>

<h2 id="segmentation approaches">Segmentation Approaches</h2>
<p>
  For subtitle generation, we experimented with multiple segmentation approaches to handle long sentences and ensure proper alignment of subtitles. 
</p>
<h3>
  Gemini 1.5 Pro
</h3>
<p>
  We used the full transcript from the Whisper-Timestamped model as input for Gemini 1.5 Pro, instructing it to segment the text \textbf{without modifying the extracted content}. While Gemini is effective at generating accurate segments in its standard form, we encountered challenges with longer videos (over 5 minutes in duration), where it tended to produce hallucinations or repeat the original text without proper segmentation.
</p>

<h3>
  SaT(Segment Any Text model)
</h3>
<p>
  Segment Any Text (SAT) is a fast, multilingual sentence segmentation model that handles noisy and short text efficiently. It uses subword tokenization, robust training with data corruption, and a limited lookahead mechanism for accurate segmentation across various domains.
</p>
<p>
  We have finetuned SaT on our <b>Custom dataset</b> to observe its performance for subtitle segmentation and alignment. 
</p>

<h3>
  Custom Segmentation Function
</h3>
<p>
  We developed a custom segmentation     function that helps in generating improved subtitles without any LLMs. To improve readability and alignment we have incorporated the following into our function:
</p>
<ol>
  <li><b>Splitting long segments:</b> Splits long speech captions into smaller, manageable parts for subtitle generation. Divides captions based on punctuation (e.g., commas, periods). Further splits phrases if they exceed a specified word limit (<b><code>max_words</code></b>).</li>
  <li><b>Timestamp Synchronization and Optimization:</b> Distributes timestamps proportionally based on the word count in each segment.</li>
  <li><b>Identifying pauses:</b> Inserts pauses between segments if a gap between them exceeds the specified <b><code>gap_threshold</code></b>. Creates new subtitle segments at detected pauses that exceed the threshold for better synchronization.</li>
</ol>
<div style="text-align: center;">
    <img src="./files/prompt1.png" style="max-width: 50%; height: auto;">
</div>
<p style="text-align: center;">Prompt used with LLMs for segmentation: Combining Role-play and Contextual Fusion</p>

<div class="wrapper">
  <h3 id="anticipated-challenges">Anticipated Challenges</h3>
  <p>
    We encountered the following major challenges during our research:
  </p>
  <ol>
    <li><b>SRT Input Issues:</b> LLMs like Gemini 1.5 Pro and GPT-40 altered timestamps when processing SRT files, causing misaligned and desynchronized subtitles.</li>
    <li><b>Processing long videos:</b> For videos over five minutes, LLMs struggled with segmentation accuracy, frequently hallucinating or repeating text input or given prompt.</li>
    <li><b>Unstable Output from LLMs:</b> Outputs from GPT-40 and Gemini were inconsistent, which necessitate the need for advanced prompting techniques.</li>
    <li><b>SaT Limitations:</b> Even after fine-tuning, SaT produced overly long segments unsuitable for subtitles, requiring extensive SRT data beyond our resources.</li>
  </ol>

  

  <h3>Scientific Novelty in Approach to Address Challenges</h3>
  <ol>
      <li>
          <b>Optimized LLM Usage:</b> Advanced prompting (role-playing, contextual fusion- prompt shown in Figure above) 
          and chunking via asyncio improved Gemini's segmentation stability and efficiency.
      </li>
  
      <li>
          <b>Custom Segmentation Function:</b> We created a lightweight, non-LLM approach to split captions dynamically, 
          optimize timestamps, and detected pauses for better subtitle readability and alignment.
      </li>
  
      <li>
          <b>Fine-Tuning Insights:</b> Highlighted the limitations of pre-trained models like SaT in subtitle segmentation, 
          validating lightweight custom solutions as resource-efficient alternatives.
      </li>
  </ol>

  <h3>
    Hypothesis
  </h3>
  <p>
    Our hypothesis is that combining custom segmentation techniques with fine-tuned STT models will outperform standalone STT models for subtitle generation. By refining segmentation based on punctuation, gaps, and timestamps, this hybrid approach aims to improve alignment and accuracy, especially for accents, proper nouns, and fast speech.
  </p>



  <h2 id="potential-impact">Potential Impact</h2>
  <p>
    This project aims to improve accessibility for deaf and hard-of-hearing viewers, enhance comprehension in educational contexts, and help content creators reach global audiences with precise English subtitles. Accurate captions can increase viewer engagement, boost SEO, and ensure a seamless user experience. Additionally, the project could advance AI and NLP technologies by refining captioning methodologies, benefiting video content quality across platforms.
  </p>

  <h3>Ground Truth Examples</h3>
  <div style="display: flex; justify-content: space-between; align-items: center; gap: 20px;">
      <div style="text-align: center; flex: 1;">
          <img style="height: 300px; width: auto;" alt="Ground truth example 1" src="./files/data1.png">
      </div>
      <div style="text-align: center; flex: 1;">
          <img style="height: 300px; width: auto;" alt="Ground truth example 2" src="./files/data2.png">
      </div>
      <div style="text-align: center; flex: 1;">
          <img style="height: 300px; width: auto;" alt="Ground truth example 3" src="./files/data3.png">
      </div>
  </div>
  <p style="text-align: center;"><b>Examples of ground truth SRT files</b></p>

  <hr>
</div>



<div class="wrapper">
  <h2 id="novelty">Novelty</h2>
  <p>
    Our project addresses limitations in Whisper's transcription process, focusing on improving spoken word accuracy and subtitle readability. Whisper, while robust, often struggles with punctuation, grammar, and contextual understanding, leading to less polished subtitles.
  </p>
  <p>
    To enhance accuracy, we integrate Whisper with LLMs like Gemini-1.5 and GPT-40, for improving segmentation of long sentences. By experimenting with different prompting techniques we are able to be coherent captions with correct grammar and punctuation for relevant video frames. 
  </p>
  <p>
    We also introduce a custom segmentation function to split long speech segments into concise, contextually meaningful chunks, improving readability and comprehension. Our approach delivers more accurate, user-friendly subtitles, addressing critical challenges in transcription and subtitling for media, education, and accessibility.
  </p>

  <div style="text-align: center;">
    <img src="./files/workflow.png" style="max-width: 60%; height: auto;">
  </div>

</div>

<hr>

<h2>
  Experiments and Results
</h2>
<h3>
  1. Semantic Accuracy:
</h3>
<p>
  We assess the final SRTs obtained after applying LLM/custom function segmentation w.r.t. GT files using:
</p>

<ul>
  <li>
      <b>Character Error Rate:</b> CER value indicates the percentage of characters that were incorrectly predicted w.r.t ground truth. 
      The lower the value, the better the performance of the ASR system with 0 being a perfect score.
      
      <div style="text-align: center;">
          <img src="./files/cer.png" style="width: 40%; height: auto;" alt="Character Error Rate Calculation">
          <p style="text-align: center;"><i>Character Error Rate Calculation</i></p>
      </div>
  </li>

  <li>
      <b>Word Error Rate:</b> WER is a similar metric derived from the Levenshtein distance, working at the word level instead of 
      the phoneme level. WER values range from 0 to 1, where 0 indicates that the compared pieces of text are exactly identical, 
      and 1 indicates that they are completely different.
      
      <div style="text-align: center;">
        <img src="./files/wer.png" style="width: 50%; height: auto;" alt="Word Error Rate Calculation">
        <p style="text-align: center;"><i>Word Error Rate Calculation</i></p>
    </div>
  </li>

  <li>
    Table below shows the maximum and minimum WER and CER values obtained while evaluating
    the SRTs obtained from four different methods: Initial Whisper-timestamped SRT, and then corrected
    SRTs using Gemini-1.5-pro, SaT model and our Custom Segmentation function.
    
  <div style="text-align: center;">
    <img src="./files/table_error.png" style="width: 70%; height: auto;" alt="Word Error Rate Calculation">
    <p style="text-align: center;"><i>Maximum and Minimum WER and CER
      values obtained with different methods</i></p>
  </div>
  </li>

</ul>

<ul>
  <li><b>Observations from WER/CER Table</b></li>
  <li>
      1. Whisper-timestamped transcripts exhibit high minimum and maximum error rates at both the word and character levels, reflecting significant inaccuracies.
  </li>
  <li>
      2. Using Gemini-1.5 for segmentation reduces WER and CER but retains higher error rates due to potential insertions, deletions, or modifications of words and characters in the original transcript.
  </li>
  <li>
    3. SaT model achieves the lowest error rates overall, though it tends to produce longer segments.
  </li>
  <li>
    4. The Custom Function maintains low error rates with minimal modifications, primarily in
punctuation, while ensuring segment lengths remain within the defined maximum word limit.
  </li>
  <div style="text-align: center;">
    <img src="./files/srt_wer_cer.png" style="width: 60%; height: auto;" alt="Word Error Rate Calculation">
    <p style="text-align: center;"><i>Visualizing WER/CER levels for different
      SRT outputs of Custom segment function</i></p>
  </div>
</ul>


<h3>
  2. Performance of Different Segmentation Methods:
</h3>
<ul>
  <li>
    <b>Segment Length Comparison:</b> Quantitatively comparing segment lengths generated by different methods 
    (Whisper-timestamped, SaT, Gemini-1.5, Custom function) to assess readability. While segment alignment 
    with the ground truth SRT was checked, varying segment counts across methods made direct matching difficult. 
    Visual inspection on video was required to evaluate alignment effectively.
  </li>
</ul>

<h3>
  3. Human Evaluation:
</h3>
<p>
  Human observation remains the most effective method to assess subtitle quality. This involves checking how well 
  the subtitles align with the spoken words, ensuring the displayed text matches the speech, and verifying that 
  subtitles appear on-screen in a timely manner, synchronized with the dialogue.
</p>

<hr>

<h2>Results visualization</h2>
<h3>Displaying initial Whisper transcription results</h3>

<div style="text-align: center; margin: 20px 0;">
  <table style="margin: auto; border-collapse: collapse;">
    <tr>
      <td style="padding: 10px; text-align: center;">
        <span style="display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: white; border: 1px solid black;"></span>
        <br>
        <strong>Ground Truth</strong>
      </td>
      <td style="padding: 10px; text-align: center;">
        <span style="display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: blue;"></span>
        <br>
        <strong>Whisper-Medium</strong>
      </td>
      <td style="padding: 10px; text-align: center;">
        <span style="display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: yellow;"></span>
        <br>
        <strong>Whisper-Timestamped</strong>
      </td>
      <td style="padding: 10px; text-align: center;">
        <span style="display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: violet;"></span>
        <br>
        <strong>Whisper-X</strong>
      </td>
    </tr>
  </table>
</div>

<div style="text-align: center;">
  <img src="./files/res1.png" style="max-width: 50%; height: auto;" alt="Initial transcription results">
  <p><i>Initial transcription results - color coding represents the Whisper model used</i></p>
</div>

<h3>Comparison of Segment Lengths in a Video</h3>
<p>
  This illustrates the performance of different segmentation methods and their alignment with ground truth subtitles. 
  Each method produces varying lengths of segments, thus the total number of segments also varies.
</p>

<div style="text-align: center;">
  <img src="./files/segment_length.png" style="max-width: 50%; height: auto;" alt="Segment length comparison">
  <p><i>Comparing Segments across different Text Segmentation methods</i></p>
</div>

<h3>Qualitative Analysis of Results:</h3>
<p>
  Visually comparing outputs (example in figures below) revealed that the custom segmentation function closely matches 
  Gemini's output, with only minor transcription differences in Gemini. However, like all LLMs, Gemini occasionally 
  modifies the original transcript through word insertions or deletions.
</p>
<p>
  The custom function demonstrated the most consistent performance, adhering to a maximum segment length of 8 words 
  and maintaining sentence integrity and alignment.
</p>

<div style="display: flex; flex-direction: row; gap: 20px; justify-content: center;">
  <div style="text-align: center;">
    <img src="./files/out1.png" style="height: 3.5cm; width: 7cm;" alt="SaT output">
    <p><i>SaT output</i></p>
  </div>
  
  <div style="text-align: center;">
    <img src="./files/out2.png" style="height: 3.5cm; width: 7cm;" alt="Gemini-1.5 output">
    <p><i>Gemini-1.5 output</i></p>
  </div>
  
  <div style="text-align: center;">
    <img src="./files/out3.png" style="height: 3.5cm; width: 7cm;" alt="Custom function">
    <p><i>Custom function</i></p>
  </div>
</div>

<p>
  While Gemini and GPT perform comparably, SaT remains unsatisfactory despite being specifically trained for segmentation. 
  It produces excessively long segments, making the subtitles unsuitable for viewing, highlighting the limitations of its 
  current implementation for practical use.
</p>
  
<hr>

<h2>Error Analysis</h2>

<p>
  Figure below illustrates a common issue with LLMs: the unintended insertion or deletion of words in the transcript.
</p>

<div style="text-align: center;">
  <img src="./files/sample.png" style="max-width: 50%; height: auto;" alt="Results comparison">
  <p><i>Results of each method for the same paragraph.</i></p>
</div>

<hr>

<h2>Discussion</h2>

<h3>Replicability</h3>
<p>
  This work is easily replicable due to the detailed methodology and use of publicly available tools and datasets.
</p>

<h4 style="text-align: left;">Dataset Contribution</h4>
<p>
  The custom dataset, sourced from Euro Brady's YouTube series and enriched with manual refinements, includes accurate SRTs 
  that serve as a robust baseline for benchmarking subtitle generation models. Its availability encourages further research 
  into improving transcription accuracy across diverse audio-visual content.
</p>

<h4 style="text-align: left;">Ethical Considerations</h4>
<p>
  While the dataset comprises publicly available content, privacy and copyright laws must be respected. Inaccuracies in 
  transcription, particularly in sensitive contexts, could lead to misinformation. Transparency about limitations and human 
  verification for critical use cases are necessary to mitigate risks.
</p>

<h4 style="text-align: left;">Streamlit GUI</h4>
<p>
  We developed an end-to-end pipeline integrated
  into a user-friendly Streamlit GUI to demonstrate
  the Speech to text transcription and Subtitle generation
  process. Users can seamlessly input a YouTube
  URL or upload a video file, extract audio, and generate
  captions using Whisper-timestamped model
  and Segmentation function.
</p>
<div style="text-align: center;">
  <img src="./files/gui1.png" style="max-width: auto; height: auto;" alt="Results comparison">
  <p><i>GUI display: processing a YouTube video and correcting the subtitles</i></p>
</div>
<p>
Figure above shows the display of GUI, in the left panel link to YouTube video link can be pasted. A video in mp4 format could also be uploaded. 
It also shows the process of downloading audio /video from a YouTube link given as input and generating the
improved SRT with custom segmentation approach for the final subtitles. 
</p>

<div style="text-align: center;">
  <img src="./files/gui2.png" style="max-width: auto; height: auto;" alt="Results comparison">
  <p><i>GUI display: processing a YouTube video and correcting the subtitles</i></p>
</div>
<p>
Finally, the refined SRT files' subtitles are embedded into the
video using FFmpeg, creating a captioned video
output. With options to preview the video and
download the SRT file, the pipeline offers a complete
solution for showcasing the workflow and
results in an interactive demo (see figure above).
</p>


<h4 style="text-align: left;">Future Work</h4>
<p>
  Future research could focus on fine-tuning the Whisper/SaT models on diverse datasets for better accent recognition and 
  handling complex audio environments. Implementing advanced noise-canceling techniques and enhancing the contextual 
  understanding of speech could further improve accuracy.
</p>

<p>
  Also as suggested during the poster presentation, future work could involve <strong><em>comparing the audio signal with 
  timestamps and transcribed subtitles to evaluate how accurately subtitles align with the speech flow</em></strong>. 
  However, this task is complex, as it requires simultaneously analyzing audio signals, word alignment, and timestamp 
  accuracy. Due to time constraints, this approach could not be implemented in the current project but remains a valuable 
  direction for further research.
</p>

<hr>

<h2>Conclusion</h2>

<h3>Summary of Findings</h3>
<p>
  This project improved subtitle accuracy by enhancing STT outputs with a custom segmentation function, achieving better word 
  accuracy, punctuation, and readability. Metrics like WER and BLEU demonstrated the proposed approach's superiority over 
  baseline models, such as Whisper-Medium and Gemini.
</p>

<h3>Significance of Results</h3>
<p>
  Combining Whisper-timestamped with VAD feature transcription with multiple segmentation techniques (utilizing LLMs/ our 
  custom segmentation function) proved effective in generating accurate, readable subtitles, addressing challenges like 
  accents, fast speech, and noise. These findings contribute to advancing AI and NLP for automated transcription.
</p>

<h3>Potential Applications</h3>
<p>
  The system has broad applications in media, education, accessibility, multilingual content creation, and corporate 
  communication, enhancing transcription accuracy and user experience across diverse industries.
</p>

<hr>

<div class="wrapper">
  <h2 id="limitation">Limitations</h2>
  <ol>
    <li>Segmentation and Alignment: Difficulty in breaking sentences into readable segments often results in misaligned captions.</li>
    <li>Overlapping Speech: Simultaneous dialogue complicates voice differentiation.</li>
    <li>Background Noise: Environmental sounds interfere with speech clarity.</li>
    <li>Contextual Understanding: Limited grasp of context can result in misleading captions.</li>
    <li>Technical Constraints: Real-time processing and scalability across platforms are significant barriers.</li>
    <li>User Experience: Poor timing or inaccuracies reduce viewer engagement.</li>
    <li>Quality Consistency: Maintaining readability and accuracy across diverse speakers is difficult.</li>
  </ol>
  <hr>
</div>

</body></html>
